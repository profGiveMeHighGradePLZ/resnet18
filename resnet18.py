# -*- coding: utf-8 -*-
"""resnet18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Wnsofq918nMiUI5ARa-R51xmgRxUh-O
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets,utils
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data.dataset import Dataset
from torchvision.transforms import transforms
from pathlib import Path
import cv2
from PIL import Image
import torch.nn.functional as F

"""# 导入cifar10数据库"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
#确保 matplotlib 的图表会内嵌显示在 Jupyter Notebook 中，而不是弹出一个单独的窗口。
# %config InlineBackend.figure_format = 'svg'
#将内嵌图表的格式配置为 SVG（可缩放矢量图形）格式

transform = transforms.Compose(
    [ToTensor(),
     transforms.Normalize(mean = [0.5,0.5,0.5],std = [0.5,0.5,0.5]),
     transforms.Resize((224,224))]
)

training_data = datasets.CIFAR10(root = './data',train = True,download = True,transform = transform)
testing_data = datasets.CIFAR10(root = './data',train = False,download = True,transform = transform)

"""# 构建神经网络，定义损失函数

构建残差块
"""

class BasicBlock(nn.Module):
    def __init__(self,in_channels,out_channels,stride=[1,1],padding=1) -> None:
        super(BasicBlock, self).__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride[0],padding=padding,bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=stride[1],padding=padding,bias=False),
            nn.BatchNorm2d(out_channels)
        )

        self.shortcut = nn.Sequential()
        if stride[0] != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride[0], bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.layer(x)
        out += self.shortcut(x)
        out = F.relu(out)
        return out

"""构建神经网络"""

class ResNet18(nn.Module):
    def __init__(self, BasicBlock, num_classes=10) -> None:
        super(ResNet18, self).__init__()
        self.in_channels = 64
        self.conv1 = nn.Sequential(
            nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        self.conv2 = self._make_layer(BasicBlock,64,[[1,1],[1,1]])

        self.conv3 = self._make_layer(BasicBlock,128,[[2,1],[1,1]])

        self.conv4 = self._make_layer(BasicBlock,256,[[2,1],[1,1]])

        self.conv5 = self._make_layer(BasicBlock,512,[[2,1],[1,1]])

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, block, out_channels, strides):
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)
    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.conv5(out)

        # out = F.avg_pool2d(out,7)
        out = self.avgpool(out)
        out = out.reshape(x.shape[0], -1)
        out = self.fc(out)
        return out

res18 = ResNet18(BasicBlock)
print(res18)

batch_size = 100
train_data = DataLoader(dataset = training_data,batch_size = batch_size, shuffle = True,drop_last = True)
test_data = DataLoader(dataset = testing_data,batch_size = batch_size, shuffle = True,drop_last = True)

images,labels = next(iter(train_data))
print(images.shape)
img = utils.make_grid(images)
img = img.numpy().transpose(1,2,0)
#tensor转化
mean = [0.5,0.5,0.5]
std = [0.5,0.5,0.5]
img = img*std + mean
print([labels[i] for i in range(64)])
plt.imshow(img)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = res18.to(device)

#定义损失函数
cost = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

print(len(train_data))
print(len(test_data))

"""# 训练,测试"""

epochs = 10
for epoch in range(epochs):
    running_loss = 0.0
    running_correct = 0.0
    model.train()
    print("Epoch {}/{}".format(epoch+1,epochs))
    print("-"*10)
    for X_train,Y_train in train_data:
        X_train,Y_train = X_train.to(device),Y_train.to(device)
        outputs = model(X_train)
        _,pred = torch.max(outputs.data,1)
        optimizer.zero_grad()
        loss = cost(outputs,Y_train)

        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        running_correct += torch.sum(pred == Y_train.data)

    testing_correct = 0
    test_loss = 0
    model.eval()
    for x_test,y_test in test_data:
        x_test,y_test = x_test.to(device),y_test.to(device)
        outputs = model(x_test)
        loss = cost(outputs,y_test)
        _,pred = torch.max(outputs.data,1)
        testing_correct += torch.sum(pred == y_test.data)
        test_loss += loss.item()
    print("Train Loss is:{:.4f},Train Accuracy is:{:.4f}%,Test Loss is::{:4f}Test Accuracy is:{:.4f}%".format(
        running_loss/len(training_data),100*running_correct/len(training_data),
        test_loss/len(testing_data),
        100*testing_correct/len(testing_data)
    ))